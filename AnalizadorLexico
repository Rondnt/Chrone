# ------------------------------------------------------------
# calclex.py
#
# tokenizer for a simple expression evaluator for
# numbers and +,-,*,/
# ------------------------------------------------------------
import ply.lex as lex

# r'atring' -> r significa que la cadena es tradada sin caracteres de escape, 
# es decir r'\n' seria un \ seguido de n (no se interpretaria como salto de linea) 

# List of token names.   This is always required

tokens = (
    'NUMERO',
    'IDENTIFICADOR',
    'CADENA',
    'ESPACIO',
    'INICOMENTARIO',
    'FINCOMENTARIO',
    'GUIONES',
    'PUNTO',
    'COMA',
    'ENTERO',
    'DECIMAL',
    'CARACTER',
    'SUMA',
    'RESTA',
    'MULT',
    'DIV',
    'ASIGNACION',
    'SALIDA',
    'ENTRADA',
    'MENOR',
    'MAYOR',
    'MODULO',
    'MAYORIGUAL',
    'MENORIGUAL',
    'COMPARAR',
    'NEGACION',
    'DISTINTO',
    'IPAREN',
    'DPAREN',
    'ILLAVE',
    'DLLAVE',
    'FINSENTENCIA',
)
 
 # Regular expression rules for simple tokens

t_IDENTIFICADOR = r'[a-zA-Z_][a-zA-Z0-9_]*'
#t_CADENA = r'""'
t_ESPACIO = r'\s+'
t_INICOMENTARIO = r'/*'
t_FINCOMENTARIO = r'*/'
#t_GUIONES = r'-_'
t_PUNTO = r'\.'
t_COMA = r','
t_ENTERO = r'[1-9][0-9]*'
t_DECIMAL = r'[0-9][0-9]+'
#t_CARACTER= r''
t_SUMA    = r'\+'
t_RESTA   = r'-'
t_MULT   = r'\*'
t_DIV  = r'/'
t_IPAREN  = r'\('
t_DPAREN  = r'\)'
t_ILLAVE = r'\{'
t_DLLAVE = r'\}'
#t_NUMBER  = r'\d+'
 
 # A regular expression rule with some action code

def t_NUMERO(t):
    r'\d+'
    t.value = int(t.value)  # guardamos el valor del lexema  
    #print("se reconocio el numero")
    return t
  
 #def t_IDENTIFICADOR(t):
  #r'[a-zA-Z_]+([a-zA-Z0-9_]*)'
 #  return t
 # Define a rule so we can track line numbers
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)
 
 # A string containing ignored characters (spaces and tabs)
t_ignore  = ' \t'
 
 # Error handling rule
def t_error(t):
    print("Illegal character '%s'" % t.value[0])
    t.lexer.skip(1)
 
# Build the lexer
lexer = lex.lex()

# Test it out
data = ''' 2+3*  , 4(2-3) .  
 i /* */'''

 
# Give the lexer some input
lexer.input(data)

# Tokenize
while True:
    tok = lexer.token()
    if not tok: 
        break      # No more input
    #print(tok)
    print(tok.type, tok.value, tok.lineno, tok.lexpos)
